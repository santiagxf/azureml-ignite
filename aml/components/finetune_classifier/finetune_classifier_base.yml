name: finetuning_classifier_base
display_name: Finetune HuggingFace Classifier
version: 8
type: command
description: |-
  **Finetune HuggingFace Classifier**:

  Allows to get a given HuggingFace base model and use it as the base model to construct a text classification model. There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch.

  When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique.

  To use this component you will need:
  * `base_weights`: The base weights of the model.
  * `base_tokenizer`: The base tokenizer used with you model.
  * `base_config` (optional): The base configuration to use for you model. Only indicate this if you model is already a classification model that you want to fine tune further.
  * `train_path`: The path where you training data is.
  * `validation_path`: The path where your validation data is. If not indicated, we will use the training data.
  * `text_column_name`: The name of the column in the dataset where the text data is placed.
  * `label_column_name`: The name of the column in the dataset where the label to predict is placed.
  * `num_labels`: The number of labels the classifier should be predicting.
  * `batch_size` (optional): The batch size to use during training.
  * `num_train_epochs` (optional): The number of epochs to train on.
  * `eval_strategy` (opional): The evaluation strategy to use during training, if any.

  The fine-tuned model can be used directly for inference if needed. It is also logged in MLflow format in the output named `classifier`. The model returns two columns `class` and `probabilities`.
tags: { category: HuggingFace, type: Classifier }
inputs:
  base_weights: 
    type: uri_folder
    description: The model checkpoint for weights initialization.
  base_tokenizer:
    type: uri_folder
    description: Pretrained tokenizer name or path if not from model_name.
  base_config: 
    type: uri_folder
    description: Pretrained model configuration.
    optional: true
  train_path: 
    type: uri_folder
    description: Directory containing pre-processed training data.
  validation_path: 
    type: uri_folder
    description: Directory containing pre-processed validation data.
    optional: true
  text_column_name: 
    type: string
    description: Name of the column containing text.
  label_column_name: 
    type: string
    description: Name of the column containing text.
  num_labels: 
    type: integer
    description: Number of labels/classes in the target.
  batch_size: 
    type: integer
    description: Batch size per step on each device.
    default: 26
  num_train_epochs: 
    type: integer
    description: Number of training epochs.
    default: 20
  eval_strategy:
    type: string
    description: Evaluation strategy
    enum: ['epoch','steps','no']
outputs:
  weights:
    type: uri_folder
  tokenizer:
    type: uri_folder
  config:
    type: uri_folder
code: .
environment: azureml://registries/IgniteContoso/environments/base-hf-transformer-trainer/versions/1
command: >-
  jobtools finetune_classifier.py finetune 
    --weights-path ${{inputs.base_weights}} 
    --tokenizer-path ${{inputs.base_tokenizer}} 
    [--config-path ${{inputs.base_config}}] 
    --train-path ${{inputs.train_path}} 
    [--validation-path ${{inputs.validation_path}}] 
    --text-column-name ${{inputs.text_column_name}} 
    --label-column-name ${{inputs.label_column_name}} 
    --num-labels ${{inputs.num_labels}} 
    --batch-size ${{inputs.batch_size}} 
    --num-train-epochs ${{inputs.num_train_epochs}} 
    --eval-strategy ${{inputs.eval_strategy}}
    --ort False 
    --fp16 False 
    --deepspeed False 
    --weights-output ${{outputs.weights}} 
    --tokenizer-output ${{outputs.tokenizer}} 
    --config-output ${{outputs.config}}